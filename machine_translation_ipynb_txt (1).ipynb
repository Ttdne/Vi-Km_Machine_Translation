{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 86023,
          "databundleVersionId": 11376393,
          "sourceType": "competition"
        },
        {
          "sourceId": 8218776,
          "sourceType": "datasetVersion",
          "datasetId": 4871830
        },
        {
          "sourceId": 8300737,
          "sourceType": "datasetVersion",
          "datasetId": 4746046
        },
        {
          "sourceId": 12601577,
          "sourceType": "datasetVersion",
          "datasetId": 7959572
        },
        {
          "sourceId": 12602387,
          "sourceType": "datasetVersion",
          "datasetId": 7960136
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\" # \"0,1,2,3\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-07-28T15:40:44.115412Z",
          "iopub.execute_input": "2025-07-28T15:40:44.116147Z",
          "iopub.status.idle": "2025-07-28T15:40:44.119659Z",
          "shell.execute_reply.started": "2025-07-28T15:40:44.116114Z",
          "shell.execute_reply": "2025-07-28T15:40:44.118847Z"
        },
        "trusted": true,
        "id": "er924s3g5xBe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install library**\n"
      ],
      "metadata": {
        "id": "5sA7bhGe5xB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets trl bitsandbytes"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T02:35:15.713621Z",
          "iopub.execute_input": "2025-08-09T02:35:15.713847Z",
          "iopub.status.idle": "2025-08-09T02:36:23.481079Z",
          "shell.execute_reply.started": "2025-08-09T02:35:15.713824Z",
          "shell.execute_reply": "2025-08-09T02:36:23.480452Z"
        },
        "id": "D1SRPGUs5xB-",
        "outputId": "5ea29997-f10c-4728-a313-12fe6e6f929d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.9/511.9 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m117.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model, Tokenizer**"
      ],
      "metadata": {
        "id": "0bwtCkjA5xCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"sail/Sailor2-1B-Chat\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.padding_side = \"left\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T02:36:34.131440Z",
          "iopub.execute_input": "2025-08-09T02:36:34.131979Z",
          "iopub.status.idle": "2025-08-09T02:37:07.593867Z",
          "shell.execute_reply.started": "2025-08-09T02:36:34.131950Z",
          "shell.execute_reply": "2025-08-09T02:37:07.593266Z"
        },
        "id": "TYodjiEh5xCJ",
        "outputId": "b81d85d7-a3cc-4090-fee7-48968637177a",
        "colab": {
          "referenced_widgets": [
            "a40eee4ead5647d3aac7a891e43e10b6",
            "fa70d287318c47ca97853a9f7157beda",
            "dbefe965990e4087b3bb7ad92b17d50e",
            "87acabdf3ff44092b9b6651bd861971f",
            "aba2a0a647154f35a9dd93ad99f1b8d4",
            "e064556b8fc04ad1bdccd2f0e0edfaef",
            "aabf6e59828844e2abf20af46a0f1d29",
            "3d43c1ed5ac04bed85bcc19648f24978",
            "aeca475719f5418e91a04448bc8a4a48"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/658 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a40eee4ead5647d3aac7a891e43e10b6"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "2025-08-09 02:36:45.159691: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754707005.311013     100 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754707005.353202     100 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa70d287318c47ca97853a9f7157beda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbefe965990e4087b3bb7ad92b17d50e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87acabdf3ff44092b9b6651bd861971f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aba2a0a647154f35a9dd93ad99f1b8d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e064556b8fc04ad1bdccd2f0e0edfaef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aabf6e59828844e2abf20af46a0f1d29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d43c1ed5ac04bed85bcc19648f24978"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aeca475719f5418e91a04448bc8a4a48"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LoRA config**"
      ],
      "metadata": {
        "id": "GF9clsiH5xCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T02:49:28.374301Z",
          "iopub.execute_input": "2025-08-09T02:49:28.374768Z",
          "iopub.status.idle": "2025-08-09T02:49:28.807495Z",
          "shell.execute_reply.started": "2025-08-09T02:49:28.374742Z",
          "shell.execute_reply": "2025-08-09T02:49:28.806903Z"
        },
        "id": "nUbDjM_B5xCR",
        "outputId": "c866653f-07a2-4193-8d43-8afe726e9daa"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T02:49:34.322741Z",
          "iopub.execute_input": "2025-08-09T02:49:34.323367Z",
          "iopub.status.idle": "2025-08-09T02:49:34.334826Z",
          "shell.execute_reply.started": "2025-08-09T02:49:34.323345Z",
          "shell.execute_reply": "2025-08-09T02:49:34.334272Z"
        },
        "id": "hJnc3AsS5xCW",
        "outputId": "ae044c85-fd39-4324-de45-de5d3c1e9baf"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "trainable params: 17,596,416 || all params: 1,005,661,056 || trainable%: 1.7497\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset**"
      ],
      "metadata": {
        "id": "VzdEuTTB5xCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('/kaggle/input/mydata/train.csv', encoding=\"utf-8\")\n",
        "train_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T02:37:30.447614Z",
          "iopub.execute_input": "2025-08-09T02:37:30.448298Z",
          "iopub.status.idle": "2025-08-09T02:37:33.085280Z",
          "shell.execute_reply.started": "2025-08-09T02:37:30.448271Z",
          "shell.execute_reply": "2025-08-09T02:37:33.084739Z"
        },
        "id": "wXn7dGFH5xCc",
        "outputId": "727c8ffe-2bb5-4c52-b4e0-24b11d4b4e54"
      },
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                                  vi  \\\n0  Ngoài ra, Bộ đang xây dựng phương thức đánh gi...   \n1  Để đạt được những kết quả đó đòi hỏi tỉnh Hưng...   \n2  Nó có nhiều màu, mỗi màu lại có một vị, mà ăn ...   \n3  Về giám sát lấy phiếu tín nhiệm, các đại biểu ...   \n4  Nhưng mọi thử nghiệm trên trường quốc tế, đặc ...   \n\n                                                  km  \n0  ក្រៅពីនោះ ក្រសួងកំពុងកសាងមធ្យោបាយវាយតម្លៃការប្...  \n1  ការសម្រេចបាននូវលទ្ធផលទាំងនោះតម្រូវឱ្យខេត្ត Hun...  \n2  វាមានពណ៌ជាច្រើន ពណ៌នីមួយៗមានរសជាតិឆ្ងាញ់ប៉ុន្ត...  \n3  ទាក់ទងនឹងការឃ្លាំមើលការបោះឆ្នោតទំនុកចិត្ត សមាជ...  \n4  ប៉ុន្តែ រាល់ការសាកល្បងលើឆាកអន្តរជាតិ ជាពិសេសគឺ...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vi</th>\n      <th>km</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ngoài ra, Bộ đang xây dựng phương thức đánh gi...</td>\n      <td>ក្រៅពីនោះ ក្រសួងកំពុងកសាងមធ្យោបាយវាយតម្លៃការប្...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Để đạt được những kết quả đó đòi hỏi tỉnh Hưng...</td>\n      <td>ការសម្រេចបាននូវលទ្ធផលទាំងនោះតម្រូវឱ្យខេត្ត Hun...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Nó có nhiều màu, mỗi màu lại có một vị, mà ăn ...</td>\n      <td>វាមានពណ៌ជាច្រើន ពណ៌នីមួយៗមានរសជាតិឆ្ងាញ់ប៉ុន្ត...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Về giám sát lấy phiếu tín nhiệm, các đại biểu ...</td>\n      <td>ទាក់ទងនឹងការឃ្លាំមើលការបោះឆ្នោតទំនុកចិត្ត សមាជ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Nhưng mọi thử nghiệm trên trường quốc tế, đặc ...</td>\n      <td>ប៉ុន្តែ រាល់ការសាកល្បងលើឆាកអន្តរជាតិ ជាពិសេសគឺ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "eval_df = pd.read_csv(\"/kaggle/input/mydata/test.csv\", encoding=\"utf-8\")\n",
        "eval_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T02:37:33.379730Z",
          "iopub.execute_input": "2025-08-09T02:37:33.380385Z",
          "iopub.status.idle": "2025-08-09T02:37:35.732435Z",
          "shell.execute_reply.started": "2025-08-09T02:37:33.380352Z",
          "shell.execute_reply": "2025-08-09T02:37:35.731910Z"
        },
        "id": "toGK08wP5xCg",
        "outputId": "699f325e-fe7f-4ce7-c69e-9c2eb6af0060"
      },
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                                  vi  \\\n0  Vượt qua những tác động tiêu cực do dịch COVID...   \n1  Cùng với đó, các nước thành viên sẽ tăng cường...   \n2  Hợp tác Mekong-Lan Thương vì một khu vực Mekon...   \n3  Vì vậy, phục hồi kinh tế hậu Covid-19 cũng là ...   \n4  Điều này cho thấy sự hiện diện ngày càng mạnh ...   \n\n                                                  km  \n0  ដោយជំនះពុះពារលើផលប៉ះពាល់អវិជ្ជមាន ដោយសារជំងឺរា...  \n1  ទន្ទឹមជាមួយគ្នានោះ បណ្ដាប្រទេសសមាជិកនឹងបង្កើនក...  \n2  កិច្ចសហប្រតិបត្តិការមេគង្គ-ឡានឆាងដើម្បីតំបន់មេ...  \n3  ដូច្នេះការស្ដារសេដ្ឋកិច្ចឡើងវិញក្រោយពីកូវីដ ១៩...  \n4  ប្រការនេះបង្ហាញពីវត្តមានកាន់តែខ្លាំងឡើងៗ របស់វ...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vi</th>\n      <th>km</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Vượt qua những tác động tiêu cực do dịch COVID...</td>\n      <td>ដោយជំនះពុះពារលើផលប៉ះពាល់អវិជ្ជមាន ដោយសារជំងឺរា...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Cùng với đó, các nước thành viên sẽ tăng cường...</td>\n      <td>ទន្ទឹមជាមួយគ្នានោះ បណ្ដាប្រទេសសមាជិកនឹងបង្កើនក...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Hợp tác Mekong-Lan Thương vì một khu vực Mekon...</td>\n      <td>កិច្ចសហប្រតិបត្តិការមេគង្គ-ឡានឆាងដើម្បីតំបន់មេ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Vì vậy, phục hồi kinh tế hậu Covid-19 cũng là ...</td>\n      <td>ដូច្នេះការស្ដារសេដ្ឋកិច្ចឡើងវិញក្រោយពីកូវីដ ១៩...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Điều này cho thấy sự hiện diện ngày càng mạnh ...</td>\n      <td>ប្រការនេះបង្ហាញពីវត្តមានកាន់តែខ្លាំងឡើងៗ របស់វ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_train(example):\n",
        "    # # system_prompt = (\n",
        "    #       \"Bạn là một chuyên gia dịch máy giữa tiếng Việt và tiếng Khmer. \"\n",
        "    #       \"Nhiệm vụ của bạn là dịch chính xác từng câu tiếng Việt sang tiếng Khmer. \"\n",
        "    #       \"Bạn tuyệt đối không được đưa ra bất kỳ giải thích hay bình luận nào.\"\n",
        "    #   )\n",
        "\n",
        "    prompt = \"Hãy dịch câu sau sang từ tiếng Việt sang tiếng Khmer:\\n\\\"{text}\\\"\".format(text=example[\"vi\"])\n",
        "\n",
        "    messages = [\n",
        "        # {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "        {\"role\": \"assistant\", \"content\": example[\"km\"]}\n",
        "    ]\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "    tokenized = tokenizer(text)\n",
        "\n",
        "    labels = tokenizer(text)[\"input_ids\"]\n",
        "\n",
        "    split_token=\"user\\n\"\n",
        "    split_index = text.index(split_token) + len(split_token)\n",
        "    source_len = len(tokenizer(text[:split_index])[\"input_ids\"])\n",
        "\n",
        "    labels[:source_len] = [-100] * source_len\n",
        "    # labels[:source_len] = -100\n",
        "\n",
        "    tokenized[\"labels\"] = labels\n",
        "\n",
        "\n",
        "    return tokenized"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T02:37:42.640234Z",
          "iopub.execute_input": "2025-08-09T02:37:42.640526Z",
          "iopub.status.idle": "2025-08-09T02:37:42.645515Z",
          "shell.execute_reply.started": "2025-08-09T02:37:42.640505Z",
          "shell.execute_reply": "2025-08-09T02:37:42.644914Z"
        },
        "id": "rqJzQSL35xCj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_ds = Dataset.from_pandas(train_df)\n",
        "eval_ds = Dataset.from_pandas(eval_df)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T02:38:05.649324Z",
          "iopub.execute_input": "2025-08-09T02:38:05.650012Z",
          "iopub.status.idle": "2025-08-09T02:38:07.185394Z",
          "shell.execute_reply.started": "2025-08-09T02:38:05.649989Z",
          "shell.execute_reply": "2025-08-09T02:38:07.184819Z"
        },
        "id": "lI7NGPZr5xCm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.map(preprocess_train, remove_columns=train_ds.column_names)\n",
        "eval_ds = eval_ds.map(preprocess_train, remove_columns=eval_ds.column_names)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T02:38:08.362693Z",
          "iopub.execute_input": "2025-08-09T02:38:08.363457Z",
          "iopub.status.idle": "2025-08-09T02:43:39.825215Z",
          "shell.execute_reply.started": "2025-08-09T02:38:08.363434Z",
          "shell.execute_reply": "2025-08-09T02:43:39.824678Z"
        },
        "id": "iK3H4hlA5xCp",
        "outputId": "94f45c94-e3ef-4589-eaf3-7360adce67a7",
        "colab": {
          "referenced_widgets": [
            "5a845e5e54604ba886238b104f975b89",
            "509f2f722cab428f985596e04619a4af"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/135164 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a845e5e54604ba886238b104f975b89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "509f2f722cab428f985596e04619a4af"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.filter(lambda x: len(x[\"input_ids\"]) <= 1000)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T02:43:43.382755Z",
          "iopub.execute_input": "2025-08-09T02:43:43.383440Z",
          "iopub.status.idle": "2025-08-09T02:44:44.482698Z",
          "shell.execute_reply.started": "2025-08-09T02:43:43.383414Z",
          "shell.execute_reply": "2025-08-09T02:44:44.482136Z"
        },
        "id": "ZJ9ItZwM5xCr",
        "outputId": "2df5848e-2e0c-4d5c-d048-7063186bce12",
        "colab": {
          "referenced_widgets": [
            "3a81917554d844c29846b0c81641eeca"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Filter:   0%|          | 0/135164 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a81917554d844c29846b0c81641eeca"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T00:42:21.634827Z",
          "iopub.execute_input": "2025-08-09T00:42:21.635474Z",
          "iopub.status.idle": "2025-08-09T00:42:21.638850Z",
          "shell.execute_reply.started": "2025-08-09T00:42:21.635454Z",
          "shell.execute_reply": "2025-08-09T00:42:21.638358Z"
        },
        "id": "kbT5DHlB5xCt",
        "outputId": "4a00db0f-4aac-4d69-e592-f9f7ef810bef"
      },
      "outputs": [
        {
          "execution_count": 39,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 2000\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max(len(x) for x in train_ds[\"input_ids\"])\n",
        "max_len"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T02:44:59.500527Z",
          "iopub.execute_input": "2025-08-09T02:44:59.501156Z",
          "iopub.status.idle": "2025-08-09T02:45:22.298488Z",
          "shell.execute_reply.started": "2025-08-09T02:44:59.501133Z",
          "shell.execute_reply": "2025-08-09T02:45:22.297952Z"
        },
        "id": "lHGmzgea5xCv",
        "outputId": "23c612bd-dc9a-4f42-8410-07a9866d4d14"
      },
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "1000"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "for i, x in enumerate(train_ds[\"input_ids\"]):\n",
        "    if len(x) > 900:\n",
        "        count = count+1\n",
        "print(count)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T02:19:34.341383Z",
          "iopub.execute_input": "2025-08-09T02:19:34.342019Z",
          "iopub.status.idle": "2025-08-09T02:19:50.808153Z",
          "shell.execute_reply.started": "2025-08-09T02:19:34.341997Z",
          "shell.execute_reply": "2025-08-09T02:19:50.807587Z"
        },
        "id": "FHH_mP_m5xCx",
        "outputId": "16391d2e-942f-44e2-fb00-4576872aebe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "131\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Collator**"
      ],
      "metadata": {
        "id": "-3FS65w75xCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T02:45:31.412773Z",
          "iopub.execute_input": "2025-08-09T02:45:31.413449Z",
          "iopub.status.idle": "2025-08-09T02:45:31.430103Z",
          "shell.execute_reply.started": "2025-08-09T02:45:31.413427Z",
          "shell.execute_reply": "2025-08-09T02:45:31.429587Z"
        },
        "id": "cR6RazB_5xC0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "JthpuLmK5xC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T00:54:08.280796Z",
          "iopub.execute_input": "2025-08-09T00:54:08.281461Z",
          "iopub.status.idle": "2025-08-09T00:54:08.284212Z",
          "shell.execute_reply.started": "2025-08-09T00:54:08.281439Z",
          "shell.execute_reply": "2025-08-09T00:54:08.283658Z"
        },
        "id": "kCmdz_qq5xC2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"/sailor2-sft\",\n",
        "    per_device_train_batch_size=8,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    gradient_checkpointing=True, #tiết kiệm VRAM vì k lưu lại kq trung gian\n",
        "    fp16=False, #ổn định khi train\n",
        "    weight_decay=1e-2,\n",
        "    logging_dir=\"/logging-sailor2\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    logging_steps=200,\n",
        "    report_to=\"none\",\n",
        "    optim=\"adamw_8bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    # use_cache=False\n",
        "    # packing=False\n",
        "    # completion_only_loss=True\n",
        "    # dataset_text_field=\"text\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator\n",
        "    # peft_config=lora_config,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T02:51:30.352958Z",
          "iopub.execute_input": "2025-08-09T02:51:30.353569Z",
          "iopub.status.idle": "2025-08-09T02:51:30.424971Z",
          "shell.execute_reply.started": "2025-08-09T02:51:30.353547Z",
          "shell.execute_reply": "2025-08-09T02:51:30.424442Z"
        },
        "id": "rwZcnO3n5xC3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T02:51:33.819620Z",
          "iopub.execute_input": "2025-08-09T02:51:33.820220Z",
          "iopub.status.idle": "2025-08-09T02:55:02.414580Z",
          "shell.execute_reply.started": "2025-08-09T02:51:33.820200Z",
          "shell.execute_reply": "2025-08-09T02:55:02.413608Z"
        },
        "id": "0l_QoRL95xC4",
        "outputId": "caca7460-3f75-44bb-c15a-fb9dd1de229e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='16' max='4222' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  16/4222 03:00 < 15:04:00, 0.08 it/s, Epoch 0.01/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_100/4032920361.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2236\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2238\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2239\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2580\u001b[0m                     )\n\u001b[1;32m   2581\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2582\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2584\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3843\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3845\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2553\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test**"
      ],
      "metadata": {
        "id": "sfutJ8WA5xC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q trl datasets peft bitsandbytes unsloth"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T06:15:03.135048Z",
          "iopub.execute_input": "2025-08-09T06:15:03.135236Z",
          "iopub.status.idle": "2025-08-09T06:17:36.448021Z",
          "shell.execute_reply.started": "2025-08-09T06:15:03.135219Z",
          "shell.execute_reply": "2025-08-09T06:17:36.447355Z"
        },
        "id": "sh9qn5lA5xC7",
        "outputId": "055877dd-12eb-4d9d-d85e-768d62206c23"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.9/511.9 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.8/306.8 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m130.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m131.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m133.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nfastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "# import wandb\n",
        "import torch\n",
        "import transformers\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments\n",
        "from peft import AdaLoraConfig, LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from datasets import load_dataset, Dataset\n",
        "from trl import SFTTrainer, setup_chat_format, SFTConfig\n",
        "\n",
        "# from prompt_template import translation_prompt\n",
        "from huggingface_hub import login\n",
        "\n",
        "# translation_prompt = '''#Instruction: Translate from Vietnamese to Khmer:\n",
        "# #Input (Vietnamese):\n",
        "# {}\n",
        "# #Output (Khmer):\n",
        "# '''\n",
        "translation_prompt = '''Dịch câu sau từ tiếng Việt sang tiếng Khmer: {}'''\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Parse Training Arguments\")\n",
        "    parser.add_argument('--pretrained_model_name_or_path', type=str, required=True,\n",
        "                        help='Specify model name or path to set transformer backbone, required')\n",
        "    parser.add_argument('--tokenizer_name_or_path', type=str, default=None,\n",
        "                        help='Specify tokenizer name or path. Default None, will use model_name_or_path')\n",
        "    parser.add_argument('--pretrained_peft_model_path', type=str, default=None,\n",
        "                        help='Specify pretrained peft model path to load adapter, default None')\n",
        "    parser.add_argument('--save_dir', type=str, default=None,\n",
        "                        help='Specify save dir, default None')\n",
        "    parser.add_argument('--train_data_path', type=str, required=True,\n",
        "                        help='Specify training dataset')\n",
        "    parser.add_argument('--use_lora', type=int, default=0, choices=[0,1],\n",
        "                        help='Specify lora adapter training')\n",
        "    parser.add_argument('--use_adalora', type=int, default=0, choices=[0,1],\n",
        "                        help='Specify adalora adapter training')\n",
        "    parser.add_argument('--init_r', type=int, default=12,\n",
        "                        help='Specify initial AdaLoRA rank')\n",
        "    parser.add_argument('--target_r', type=int, default=8,\n",
        "                        help='Specify target AdaLoRA rank')\n",
        "    parser.add_argument('--beta1', type=int, default=0.85,\n",
        "                        help='Specify initial AdaLoRA beta1')\n",
        "    parser.add_argument('--beta2', type=int, default=0.85,\n",
        "                        help='Specify initial AdaLoRA beta2')\n",
        "    parser.add_argument('--tinit', type=int, default=100,\n",
        "                        help='Specify number of warmup steps for AdaLoRA wherein no pruning is performed')\n",
        "    parser.add_argument('--tfinal', type=int, default=800,\n",
        "                        help='Specify fix the resulting budget distribution and fine-tune the model for tfinal steps when using AdaLoRA')\n",
        "    parser.add_argument('--deltaT', type=int, default=10,\n",
        "                        help='Specify interval of steps for AdaLoRA to update rank')\n",
        "    parser.add_argument('--lora_alpha',type=int, default=32,\n",
        "                        help='Specify Lora alpha')\n",
        "    parser.add_argument('--r', type=int, default=16,\n",
        "                        help='Specify Lora rank')\n",
        "    parser.add_argument('--lora_dropout', type=float, default=0.1,\n",
        "                        help='Specify LoRA dropout',)\n",
        "    parser.add_argument('--orth_reg_weight', type=float, default=0.1,\n",
        "                        help=\"Specify orthogonal regularization weight\")\n",
        "    parser.add_argument('--is_kbit', type=bool, default=None,\n",
        "                        help='Specify quatization load in')\n",
        "    parser.add_argument('--load_kbit', type=int, default=None, choices=[4, 8],\n",
        "                        help='Specify kbit training, choices [4, 8], default None')\n",
        "    parser.add_argument('--torch_dtype', type=str, default=\"fp32\", choices=['auto', 'fp32', 'fp16', 'bf16'],\n",
        "                        help='Specify torch_dtype from [`auto`, `fp32`, `fp16`, `bf16`], default fp32')\n",
        "    parser.add_argument('--fp16', type=int, default=0, choices=[0, 1],\n",
        "                        help='Specify fp16, choices [0, 1], default None')\n",
        "    parser.add_argument('--bf16', type=int, default=1, choices=[0, 1],\n",
        "                        help='Specify bf16, choices [0, 1], default None')\n",
        "    parser.add_argument('--num_train_epochs', type=int, default=2,\n",
        "                        help='Specify epochs, default 2')\n",
        "    parser.add_argument('--learning_rate', type=float, default=1e-4,\n",
        "                        help='Specify learning rate')\n",
        "    parser.add_argument('--lr_scheduler_type', type=str, default='cosine',\n",
        "                        help='Specify learning scheduler type')\n",
        "    parser.add_argument('--optim', type=str, default='adamw_8bit',\n",
        "                        help='Specify optimizer')\n",
        "    parser.add_argument('--warmup_steps', type=int, default=400,\n",
        "                        help='Specify warm up step')\n",
        "    parser.add_argument('--warmup_ratio', type=float, default=0.05,\n",
        "                        help='Specify warm up ratio')\n",
        "    parser.add_argument('--logging_steps', type=int, default=10,\n",
        "                        help='Specify logging step')\n",
        "    parser.add_argument('--save_steps', type=int, default=400,\n",
        "                        help='Specify save step')\n",
        "    parser.add_argument('--per_device_train_batch_size', type=int, default=4,\n",
        "                        help='Specify batch size')\n",
        "    parser.add_argument('--gradient_accumulation_steps', type=int, default=8,\n",
        "                        help='Specify accumulation step')\n",
        "    parser.add_argument('--is_training', type=bool, default=True,\n",
        "                        help='Specify train mode')\n",
        "    parser.add_argument('--do_eval', type=bool, default=False,\n",
        "                        help='Specify eval mode')\n",
        "    parser.add_argument('--report_to', type=str, default='wandb',\n",
        "                        help='Specify report to environment')\n",
        "    parser.add_argument('--weight_decay', type=float, default=0.0,\n",
        "                        help='Specify weight decay to use')\n",
        "    parser.add_argument('--device', type=str, default=\"cuda:0\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "\n",
        "def prepare_train_dataset(train_data_path, tokenizer):\n",
        "    dataset = pd.read_csv(train_data_path)\n",
        "    training_data = Dataset.from_pandas(dataset)\n",
        "    def format_chat_template(row):\n",
        "        row_json = [{\"role\": \"user\", \"content\": translation_prompt.format(row['vi'])},\n",
        "                    {\"role\": \"assistant\", \"content\": row['km']}]\n",
        "        row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
        "        return row\n",
        "    training_data = training_data.map(\n",
        "        format_chat_template,\n",
        "        num_proc= 4,\n",
        "    )\n",
        "    return training_data\n",
        "\n",
        "\n",
        "def load_dataset(train_data_path):\n",
        "    dataset = pd.read_csv(train_data_path)\n",
        "    training_data = Dataset.from_pandas(dataset)\n",
        "    return training_data\n",
        "\n",
        "\n",
        "def format_instruction(dataset):\n",
        "    output_texts = []\n",
        "    for i in range(len(dataset['vi'])):\n",
        "        text = translation_prompt.format(dataset['vi'][i]) + dataset['km'][i] +'\\n'\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "class Args:\n",
        "    pretrained_model_name_or_path = \"sail/Sailor2-1B-Chat\"\n",
        "    tokenizer_name_or_path = None\n",
        "    pretrained_peft_model_path = None\n",
        "    save_dir = \"/kaggle/working/\"\n",
        "    train_data_path = \"/kaggle/input/mydata/train.csv\"\n",
        "    use_lora = 1\n",
        "    use_adalora = 0\n",
        "    init_r = 12\n",
        "    target_r = 8\n",
        "    beta1 = 0.85\n",
        "    beta2 = 0.85\n",
        "    tinit = 100\n",
        "    tfinal = 800\n",
        "    deltaT = 10\n",
        "    lora_alpha = 32\n",
        "    r = 16\n",
        "    lora_r = 16\n",
        "    lora_dropout = 0.1\n",
        "    orth_reg_weight = 0.1\n",
        "    is_kbit = True\n",
        "    load_kbit = 4\n",
        "    torch_dtype = \"bf16\"\n",
        "    fp16 = 0\n",
        "    bf16 = 1\n",
        "    num_train_epochs = 2\n",
        "    learning_rate = 1e-4\n",
        "    lr_scheduler_type = \"cosine\"\n",
        "    optim = \"adamw_8bit\"\n",
        "    warmup_steps = 400\n",
        "    warmup_ratio = 0.05\n",
        "    logging_steps = 10\n",
        "    save_steps = 2000\n",
        "    per_device_train_batch_size = 8\n",
        "    gradient_accumulation_steps = 4\n",
        "    is_training = True\n",
        "    do_eval = False\n",
        "    report_to = \"none\"\n",
        "    weight_decay = 0.0\n",
        "    device = \"cuda:0\"\n",
        "\n",
        "\n",
        "def main():\n",
        "    # args = parse_args()\n",
        "    args = Args()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_count = torch.cuda.device_count()\n",
        "    else:\n",
        "        args.device = \"cpu\"\n",
        "        gpu_count = 0\n",
        "\n",
        "    # login(token='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')\n",
        "\n",
        "    # wandb.login(key='6375852ad516569bf14a7854b53517861b4e5091', relogin=True)\n",
        "    # # run = wandb.init(\n",
        "    #     project='llama3.2-3b-mt-v2',\n",
        "    #     name='SFT-Llama3.2-3b'\n",
        "    # )\n",
        "\n",
        "    base_model = args.pretrained_model_name_or_path\n",
        "\n",
        "    if args.torch_dtype == 'fp32':\n",
        "        args.torch_dtype = torch.float32\n",
        "    elif args.torch_dtype == 'fp16':\n",
        "        args.torch_dtype = torch.float16\n",
        "    elif args.torch_dtype == 'bf16':\n",
        "        args.torch_dtype = torch.bfloat16\n",
        "\n",
        "    # load the model\n",
        "    if args.is_kbit:\n",
        "        bnb_config=BitsAndBytesConfig(\n",
        "                        load_in_4bit=args.load_kbit == 4,\n",
        "                        load_in_8bit=args.load_kbit == 8,\n",
        "                        llm_int8_threshold=6.0,\n",
        "                        llm_int8_has_fp16_weight=False,\n",
        "                        bnb_4bit_compute_dtype=args.torch_dtype,\n",
        "                        bnb_4bit_use_double_quant=True,\n",
        "                        bnb_4bit_quant_type='nf4',\n",
        "                    )\n",
        "        # model = AutoModelForCausalLM.from_pretrained(\n",
        "        #             base_model,\n",
        "        #             quantization_config=bnb_config,\n",
        "        #             device_map=args.device,\n",
        "        #             trust_remote_code=True\n",
        "        #         )\n",
        "        # model = prepare_model_for_kbit_training(model)\n",
        "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=base_model,\n",
        "            dtype=None,\n",
        "            load_in_4bit=True\n",
        "        )\n",
        "    else:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "                    base_model,\n",
        "                    device_map=args.device,\n",
        "                    torch_dtype = args.torch_dtype,\n",
        "                    trust_remote_code=True\n",
        "                )\n",
        "\n",
        "    # set up peft model\n",
        "    # if args.use_lora:\n",
        "    #     # peft_config = LoraConfig(\n",
        "    #     #     r=args.lora_r,\n",
        "    #     #     lora_alpha=args.lora_alpha,\n",
        "    #     #     lora_dropout=args.lora_dropout,\n",
        "    #     #     bias='none',\n",
        "    #     #     task_type='CAUSAL_LM',\n",
        "    #     #     target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
        "    #     # )\n",
        "    # elif args.use_adalora:\n",
        "    #     peft_config = AdaLoraConfig(\n",
        "    #         init_r=args.init_r,\n",
        "    #         target_r=args.target_r,\n",
        "    #         beta1=args.beta1,\n",
        "    #         beta2=args.beta2,\n",
        "    #         tinit=args.tinit,\n",
        "    #         tfinal=args.tfinal,\n",
        "    #         deltaT=args.deltaT,\n",
        "    #         lora_alpha=args.lora_alpha,\n",
        "    #         lora_dropout=args.lora_dropout,\n",
        "    #         target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],\n",
        "    #         orth_reg_weight=args.orth_reg_weight,\n",
        "    #     )\n",
        "\n",
        "    if args.pretrained_peft_model_path is not None:\n",
        "        print(f'Load lora weight from {args.pretrained_peft_model_path}')\n",
        "        model = PeftModel.from_pretrained(\n",
        "            base_model,\n",
        "            args.pretrained_peft_model_path,\n",
        "            torch_dtype=torch.float32 if args.is_kbit else args.torch_dtype,\n",
        "            device_map=args.device,\n",
        "            is_trainable=args.is_training\n",
        "        )\n",
        "    elif args.is_training and (args.use_adalora or args.use_lora):\n",
        "        model = FastLanguageModel.get_peft_model(\n",
        "            model,\n",
        "            r=16,\n",
        "            lora_alpha=32,\n",
        "            target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "            use_gradient_checkpointing=\"unsloth\",\n",
        "            bias=\"none\"\n",
        "        )\n",
        "        # model = get_peft_model(model, peft_config)\n",
        "        model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
        "    if tokenizer.pad_token_id is None:\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    if model.config.pad_token_id is None:\n",
        "        model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "    training_data = prepare_train_dataset(args.train_data_path, tokenizer)\n",
        "\n",
        "    device = torch.device(args.device)\n",
        "    model = model.to(device)\n",
        "    model.config.use_cache=False\n",
        "    model.config.pretraining_tp=1\n",
        "    model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        train_dataset=training_data,\n",
        "        # dataset_num_proc=4,\n",
        "\n",
        "        # peft_config=peft_config,\n",
        "        args=SFTConfig(\n",
        "            output_dir=args.save_dir,\n",
        "            lr_scheduler_type=args.lr_scheduler_type,\n",
        "            warmup_ratio=args.warmup_ratio,\n",
        "            per_device_train_batch_size=args.per_device_train_batch_size,\n",
        "            gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "            num_train_epochs=args.num_train_epochs,\n",
        "            learning_rate=args.learning_rate,\n",
        "            bf16=bool(args.bf16),\n",
        "            fp16=bool(args.fp16),\n",
        "            optim=args.optim,\n",
        "            logging_steps=args.logging_steps,\n",
        "            save_steps=args.save_steps,\n",
        "            do_eval=args.do_eval,\n",
        "            weight_decay=args.weight_decay,\n",
        "            dataset_text_field=\"text\",\n",
        "            packing=False,\n",
        "            report_to=args.report_to,\n",
        "            ddp_find_unused_parameters=False if gpu_count > 1 else None,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    model.save_pretrained(args.save_dir)\n",
        "    # wandb.finish()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T06:24:42.802424Z",
          "iopub.execute_input": "2025-08-09T06:24:42.803131Z",
          "iopub.status.idle": "2025-08-09T06:24:42.813174Z",
          "shell.execute_reply.started": "2025-08-09T06:24:42.803105Z",
          "shell.execute_reply": "2025-08-09T06:24:42.812582Z"
        },
        "id": "DrrEUidD5xC8",
        "outputId": "bae2fcd3-e6fc-4412-cd65-cac40cec8aa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Overwriting train.py\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!torchrun train.py"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T06:24:47.968746Z",
          "iopub.execute_input": "2025-08-09T06:24:47.969252Z",
          "execution_failed": "2025-08-09T06:40:36.070Z"
        },
        "id": "qqksr_k85xDC",
        "outputId": "d504b215-6f28-4d69-b0a1-e545e82cc72c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n2025-08-09 06:24:54.145356: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754720694.167946    1366 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754720694.174584    1366 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n🦥 Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.8.4: Fast Qwen2 patching. Transformers: 4.55.0.\n   \\\\   /|    NVIDIA L4. Num GPUs = 4. Max memory: 22.278 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth 2025.8.4 patched 48 layers with 48 QKV layers, 48 O layers and 48 MLP layers.\ntrainable params: 17,596,416 || all params: 1,005,661,056 || trainable%: 1.7497\nMap (num_proc=4): 100%|███████| 135164/135164 [00:10<00:00, 12695.96 examples/s]\nUnsloth: Tokenizing [\"text\"] (num_proc=2): 100%|█| 135164/135164 [01:07<00:00, 1\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 135,164 | Num Epochs = 2 | Total steps = 8,448\nO^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n \"-____-\"     Trainable parameters = 17,596,416 of 647,834,496 (2.72% trained)\n{'loss': 2.0743, 'grad_norm': 29507.349609375, 'learning_rate': 2.1276595744680853e-06, 'epoch': 0.0}\n{'loss': 1.8358, 'grad_norm': 10.182147026062012, 'learning_rate': 4.491725768321513e-06, 'epoch': 0.0}\n{'loss': 1.4848, 'grad_norm': 3.223566770553589, 'learning_rate': 6.855791962174941e-06, 'epoch': 0.01}\n{'loss': 1.2178, 'grad_norm': 2.539667844772339, 'learning_rate': 9.219858156028368e-06, 'epoch': 0.01}\n{'loss': 0.9295, 'grad_norm': 2.431497097015381, 'learning_rate': 1.1583924349881797e-05, 'epoch': 0.01}\n{'loss': 0.6684, 'grad_norm': 0.7702655792236328, 'learning_rate': 1.3947990543735227e-05, 'epoch': 0.01}\n{'loss': 0.5938, 'grad_norm': 0.5793799161911011, 'learning_rate': 1.6312056737588656e-05, 'epoch': 0.02}\n{'loss': 0.5529, 'grad_norm': 0.5669154524803162, 'learning_rate': 1.867612293144208e-05, 'epoch': 0.02}\n{'loss': 0.5519, 'grad_norm': 0.550234317779541, 'learning_rate': 2.1040189125295508e-05, 'epoch': 0.02}\n{'loss': 0.5226, 'grad_norm': 0.4931143522262573, 'learning_rate': 2.340425531914894e-05, 'epoch': 0.02}\n{'loss': 0.5305, 'grad_norm': 0.5172637104988098, 'learning_rate': 2.5768321513002363e-05, 'epoch': 0.03}\n{'loss': 0.5215, 'grad_norm': 0.52204829454422, 'learning_rate': 2.8132387706855794e-05, 'epoch': 0.03}\n{'loss': 0.5146, 'grad_norm': 955193.875, 'learning_rate': 3.0496453900709222e-05, 'epoch': 0.03}\n{'loss': 0.5123, 'grad_norm': 0.5853167772293091, 'learning_rate': 3.2860520094562646e-05, 'epoch': 0.03}\n{'loss': 0.5161, 'grad_norm': 0.5696964263916016, 'learning_rate': 3.522458628841608e-05, 'epoch': 0.04}\n{'loss': 0.4991, 'grad_norm': 0.5145540237426758, 'learning_rate': 3.75886524822695e-05, 'epoch': 0.04}\n{'loss': 0.4935, 'grad_norm': 0.5452176332473755, 'learning_rate': 3.995271867612293e-05, 'epoch': 0.04}\n{'loss': 0.4909, 'grad_norm': 0.5012229681015015, 'learning_rate': 4.231678486997636e-05, 'epoch': 0.04}\n{'loss': 0.4854, 'grad_norm': 0.5333016514778137, 'learning_rate': 4.468085106382979e-05, 'epoch': 0.04}\n{'loss': 0.4796, 'grad_norm': 0.5510479807853699, 'learning_rate': 4.704491725768322e-05, 'epoch': 0.05}\n{'loss': 0.4802, 'grad_norm': 0.47422826290130615, 'learning_rate': 4.940898345153664e-05, 'epoch': 0.05}\n{'loss': 0.4759, 'grad_norm': 0.5746052861213684, 'learning_rate': 5.177304964539007e-05, 'epoch': 0.05}\n  3%|█                                     | 225/8448 [13:51<8:19:10,  3.64s/it]",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}