{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "QkBHUzxpVn-P",
        "outputId": "2bca3e16-5348-4760-fea1-26c65fce6875"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/oov_counts_viIPA.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-953258120.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcsv_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"oovs_found.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtxt_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m      \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsv_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/oov_counts_viIPA.txt'"
          ]
        }
      ],
      "source": [
        "!pip install -q unsloth bitsandbytes datasets transformers trl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cpotrain.py\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import CPOConfig, CPOTrainer\n",
        "import os\n",
        "import torch\n",
        "import wandb\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "def prepare_dataset(ds_path):\n",
        "    dataset = pd.read_csv(ds_path)\n",
        "    train_ds = Dataset.from_pandas(dataset)\n",
        "\n",
        "    return train_ds\n",
        "\n",
        "class Args:\n",
        "    model_name = \"\"\n",
        "    output_dir = \"/kaggle/working/\"\n",
        "    dataset_path = \"/kaggle/input/cpo-data/cpo_ds.csv\"\n",
        "    learning_rate = 1e-5\n",
        "    warmup_ratio = 0.1\n",
        "    num_train_epochs = 2\n",
        "    per_device_train_batch_size = 2\n",
        "    gradient_accumulation_steps = 2\n",
        "    logging_steps = 50\n",
        "    saving_steps = 200\n",
        "    bf16 = 1\n",
        "    fp16 = 0\n",
        "    weight_decay = 0.01\n",
        "    optim = \"adamw_8bit\"\n",
        "    do_eval = False\n",
        "    lora_r = 32\n",
        "    lora_alpha = 64\n",
        "    lora_dropout = 0.05\n",
        "    report_to = \"none\"\n",
        "    lr_scheduler_type = \"cosine\"\n",
        "    torch_dtype = \"bf16\"\n",
        "    device=\"cuda:0\"\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = Args()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_count = torch.cuda.device_count()\n",
        "    else:\n",
        "        args.device = \"cpu\"\n",
        "        gpu_count = 0\n",
        "\n",
        "    if args.torch_dtype == 'fp32':\n",
        "        args.torch_dtype = torch.float32\n",
        "    elif args.torch_dtype == 'fp16':\n",
        "        args.torch_dtype = torch.float16\n",
        "    elif args.torch_dtype == 'bf16':\n",
        "        args.torch_dtype = torch.bfloat16\n",
        "\n",
        "    # wandb.login(key='01cc475b8eb6c858031cb942b721df5845926606', relogin=True)\n",
        "    # run = wandb.init(\n",
        "    #     project='sailor2-SFT',\n",
        "    #     name='SFT-sailor2-1B-lora-32-extra'\n",
        "    # )\n",
        "\n",
        "    train_ds = prepare_dataset(Args.dataset_path)\n",
        "\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=args.model_name,\n",
        "        load_in_4bit=True,\n",
        "        device_map=args.device,\n",
        "        dtype=args.torch_dtype\n",
        "    )\n",
        "\n",
        "    model = FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r=Args.lora_r,\n",
        "        lora_alpha=Args.lora_alpha,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        use_gradient_checkpointing=\"unsloth\",\n",
        "        bias=\"none\",\n",
        "    )\n",
        "\n",
        "    model.print_trainable_parameters()\n",
        "\n",
        "    # model.config.use_cache = False\n",
        "    # model.config.pretraining_tp = 1\n",
        "    model = model.to(args.device)\n",
        "    model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
        "\n",
        "    cpo_args = CPOConfig(\n",
        "        learning_rate=Args.learning_rate,\n",
        "        warmup_ratio=Args.warmup_ratio,\n",
        "        num_train_epochs=Args.num_train_epochs,\n",
        "        per_device_train_batch_size=Args.per_device_train_batch_size,\n",
        "        gradient_accumulation_steps=Args.gradient_accumulation_steps,\n",
        "        logging_steps=Args.logging_steps,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=Args.saving_steps,\n",
        "        bf16=bool(Args.bf16),\n",
        "        fp16=bool(Args.fp16),\n",
        "        weight_decay=Args.weight_decay,\n",
        "        optim=Args.optim,\n",
        "        do_eval=Args.do_eval,\n",
        "        report_to=Args.report_to,\n",
        "        lr_scheduler_type=Args.lr_scheduler_type,\n",
        "        ddp_find_unused_parameters=False if gpu_count > 1 else None,\n",
        "    )\n",
        "\n",
        "    cpo_trainer = CPOTrainer(\n",
        "        model=model,\n",
        "        train_dataset=train_ds,\n",
        "        args=cpo_args,\n",
        "        processing_class=tokenizer\n",
        "    )\n",
        "\n",
        "    cpo_trainer.train()\n",
        "    model.save_pretrained(args.save_dir)\n",
        "    wandb.finish()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        ""
      ],
      "metadata": {
        "id": "s22uiKwykpy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!torchrun cpotrain.py"
      ],
      "metadata": {
        "id": "f1XGKWe3lHeU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}